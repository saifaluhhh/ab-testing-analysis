# -*- coding: utf-8 -*-
"""AB_Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qrlFKnCealXxldbgvyZD6e1wegb2RsmJ
"""

import numpy as np
import pandas as pd
from scipy.stats import norm
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Import necessary libraries
import pandas as pd

# Step 2: Upload the file using Colab's file uploader
from google.colab import files
uploaded = files.upload()  # This will prompt you to upload a CSV

# Step 3: Load the CSV into a DataFrame
# Replace the filename with the actual name from uploaded
df_ab_test = pd.read_csv("ab_test_click_data (1).csv")

# Step 4: Preview the data
print(df_ab_test.head())
print(df_ab_test.describe())

# Step 5: Group by 'group' and summarize clicks
# If you want to sum the clicks:
print(df_ab_test.groupby("group")["click"].sum())

# Optional: Count total users per group
print(df_ab_test["group"].value_counts())

"""A simple bar chart showing the total clicks versus no-clicks in each group can provide a clear visual comparison of user engagement."""

# Custom palette for yellow and black
palette = {0: 'yellow', 1: 'black'}  # Assuming 0 is for no-click, 1 for click

# Plotting the click distribution for each group with the custom colors
plt.figure(figsize=(10, 6))
ax = sns.countplot(x='group', hue='click', data=df_ab_test, palette=palette)
plt.title('Click Distribution in Experimental and Control Groups')
plt.xlabel('Group')
plt.ylabel('Count')
plt.legend(title='Click', labels=['No', 'Yes'])

# Calculate the percentages and annotate the bars
group_counts = df_ab_test.groupby(['group']).size()
print(group_counts)
group_click_counts = df_ab_test.groupby(['group', 'click']).size().reset_index(name='count')

for p in ax.patches:
    height = p.get_height()
    # Find the group and click type for the current bar
    group = 'exp' if p.get_x() < 0.5 else 'con'
    click = 1 if p.get_x() % 1 > 0.5 else 0
    total = group_counts.loc[group]
    percentage = 100 * height / total
    ax.text(p.get_x() + p.get_width() / 2., height + 5, f'{percentage:.1f}%', ha="center", color='black', fontsize=10)

plt.tight_layout()
plt.show()

"""Define Significance Level and Minimum Detectable Effect (MDE)"""

alpha = 0.05 # 5%
print("Alpha: significance level is:", alpha )

delta = 0.1 # 10%
print("Delta: minimum detectable effect is:", delta )

"""Calculating total number of clicks per group by summing clicks"""

N_con = df_ab_test[df_ab_test["group"] == "con"].count()[0]
N_exp = df_ab_test[df_ab_test["group"] == "exp"].count()[0]

# calculating the total number of clicks per group by summing 1's
X_con = df_ab_test.groupby("group")["click"].sum().loc["con"]
X_exp = df_ab_test.groupby("group")["click"].sum().loc["exp"]

# printing this for visibility
print(df_ab_test.groupby("group")["click"].sum())
print("Number of user in Control: ", N_con)
print("Number of users in Experimental: ", N_exp)
print("Number of CLicks in Control: ", X_con)
print("Number of CLicks in Experimental: ", X_exp)

"""Calculating Pooled Estimates for Clicks per Group"""

# computing the estimate of click probability per group
p_con_hat = X_con/N_con
p_exp_hat = X_exp/N_exp
print("Click Probability in Control Group:", p_con_hat)
print("Click Probability in Experimental Group:", p_exp_hat)

# computing the estimate of pooled clicked probability
p_pooled_hat = (X_con+X_exp)/(N_con + N_exp)
print("Pooled Click Probability:", p_pooled_hat)

"""Calculating Pooled Variance"""

# computing the estimate of pooled variance
pooled_variance = p_pooled_hat * (1-p_pooled_hat) * (1/N_con + 1/N_exp)
print("p^_pooled is: ", p_pooled_hat)
print("pooled_variance is: ", pooled_variance)

"""Calculating Standard Error and Test Statistics"""

import numpy as np
from scipy.stats import norm

# Input values (replace with actual data if needed)
p_exp_hat = 0.10   # Conversion rate of experimental group
p_con_hat = 0.12   # Conversion rate of control group
n_exp = 1000       # Sample size of experimental group
n_con = 1000       # Sample size of control group

# Pooled proportion
p_pool = (p_exp_hat * n_exp + p_con_hat * n_con) / (n_exp + n_con)

# Pooled variance
pooled_variance = p_pool * (1 - p_pool) * (1 / n_exp + 1 / n_con)

# Standard error
SE = np.sqrt(pooled_variance)
print("Standard Error:", round(SE, 6))

# Z-test statistic
test_stat = (p_con_hat - p_exp_hat) / SE
print("Z-test Statistic:", round(test_stat, 4))

# Z-critical value for 95% confidence (two-tailed)
alpha = 0.05
z_crit = norm.ppf(1 - alpha / 2)
print("Z-critical value (alpha = 0.05):", round(z_crit, 4))

# Hypothesis test result
if abs(test_stat) > z_crit:
    print("Result: Statistically significant — reject the null hypothesis.")
else:
    print("Result: Not statistically significant — fail to reject the null hypothesis.")

"""**Calculating p_values of the Z-test**


*A low p-value (p ≤ 0.05 at 5% significance
level) indicates strong evidence against the null hypothesis, so we reject the null hypothesis.

*A high p-value (p > 0.05) indicates weak evidence against the null hypothesis, so we fail to reject the null hypothesis.
"""

#calculating p value
p_value = 2 * norm.sf(abs(Test_stat))

# function checking the statistical significance
def is_statistical_significance(p_value, alpha):
    """
    We assess whether there is statistical significance based on the p-value and alpha.

    Arguments:
    - p_value (float): The p-value resulting from a statistical test.
    - alpha (float, optional): The significance level threshold used to determine statistical significance. Defaults to 0.05.

    Returns:
    - Prints the assessment of statistical significance.
    """

    # Print the rounded p-value to 3 decimal places
    print(f"P-value of the 2-sample Z-test: {(p_value)}")

    # Determine statistical significance
    if p_value <= alpha:
        print("There is statistical significance, indicating that the observed differences between the groups are unlikely to have occurred by chance alone. This suggests that the changes in the experimental group have a real effect compared to the control group.")
    else:
        print("There is no statistical significance, suggesting that the observed differences between the groups could have occurred by chance. This implies that the changes in the experimental group do not have a substantial effect compared to the control group.")



is_statistical_significance(p_value, alpha)

# Parameters for the standard normal distribution
mu = 0  # Mean
sigma = 1  # Standard deviation
x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
y = norm.pdf(x, mu, sigma)


# Plotting the standard normal distribution
plt.plot(x, y, label='Standard Normal Distribution')
# Shade the rejection region for a two-tailed test
plt.fill_between(x, y, where=(x > Z_crit) | (x < -Z_crit), color='red', alpha=0.5, label='Rejection Region')
# Adding Test Statistic
plt.axvline(Test_stat, color='green', linestyle='dashed', linewidth=2, label=f'Test Statistic = {Test_stat:.2f}')
# Adding Z-critical values
plt.axvline(Z_crit, color='blue', linestyle='dashed', linewidth=1, label=f'Z-critical = {Z_crit:.2f}')
plt.axvline(-Z_crit, color='blue', linestyle='dashed', linewidth=1)

# Adding labels and title
plt.xlabel('Z-value')
plt.ylabel('Probability Density')
plt.title('Gaussian Distribution with Rejection Region \n (A/B Testing for LunarTech CTA button)')
plt.legend()
# Show plot
plt.show()

"""Calculating Confidence Interval of the test"""

# Calculate the Confidence Interval (CI) for a 2-sample Z-test
## Calculate the lower and upper bounds of the confidence interval
CI = [
    round((p_exp_hat - p_con_hat) - SE*Z_crit, 3),  # Lower bound of the CI, rounded to 3 decimal places
    round((p_exp_hat - p_con_hat) + SE*Z_crit, 3)   # Upper bound of the CI, rounded to 3 decimal places
]

# Print the calculated confidence interval
print("Confidence Interval of the 2 sample Z-test is: ", CI)

# Here, the confidence interval provides a range of values within which the true difference between the experimental and control group proportions is likely to lie with a certain level of confidence (e.g., 95%).

"""Testing for Practical Significance in A/B Testing"""

import numpy as np
from scipy.stats import norm

# Step 1: Input values
p_exp_hat = 0.10   # Conversion rate of experimental group
p_con_hat = 0.12   # Conversion rate of control group
n_exp = 1000       # Sample size of experimental group
n_con = 1000       # Sample size of control group
delta = 0.01       # Minimum Detectable Effect (MDE)
alpha = 0.05       # Significance level

# Step 2: Compute pooled proportion and standard error
p_pool = (p_exp_hat * n_exp + p_con_hat * n_con) / (n_exp + n_con)
pooled_variance = p_pool * (1 - p_pool) * (1 / n_exp + 1 / n_con)
SE = np.sqrt(pooled_variance)

# Step 3: Compute confidence interval
z_crit = norm.ppf(1 - alpha / 2)
diff = p_con_hat - p_exp_hat
CI_95 = (diff - z_crit * SE, diff + z_crit * SE)

# Step 4: Define practical significance function
def is_practically_significant(delta, CI_95):
    """
    Determines if the observed difference is practically significant based on the Minimum Detectable Effect (MDE).

    Parameters:
    - delta (float): Minimum Detectable Effect.
    - CI_95 (tuple): 95% confidence interval (lower_bound, upper_bound).

    Returns:
    - bool: True if practically significant, False otherwise.
    """
    lower_bound_CI = CI_95[0]
    if lower_bound_CI >= delta:
        print(f"Practical significance confirmed. CI lower bound ({lower_bound_CI:.4f}) exceeds MDE ({delta:.4f}).")
        return True
    else:
        print(f"No practical significance. CI lower bound ({lower_bound_CI:.4f}) is below MDE ({delta:.4f}).")
        return False

# Step 5: Evaluate
significance = is_practically_significant(delta, CI_95)
print("Lower bound of 95% Confidence Interval:", round(CI_95[0], 4))